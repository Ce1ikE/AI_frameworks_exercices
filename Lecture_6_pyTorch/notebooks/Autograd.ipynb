{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2db3c02",
   "metadata": {},
   "source": [
    "# Autograd\n",
    "\n",
    "In search of an optimal solution for a given problem we will need to tweak the parameters of the neural network model. Knowing how to tweak the parameters and knowing to either increase or decrease a specific value, translates into math in finding a partial derivative of our model. This operation is the central part of backpropagation used within neural networks.\n",
    "\n",
    "PyTorch has the **autograd** feature that helps in these calculations. It will allow to build flexible and fast machine learning projects. Autograd will allow the computation of multiple partial derivatives (also named *gradients*) of our complex model. \n",
    "\n",
    "The power of autograd comes from the fact that it traces your computation dynamically *at runtime,*. This means that if your model has decision branches, or loops whose lengths are unknown up to the moment the model will execute - or run - autograd will still trace these calculations correctly, and will calculate the correct gradients to drive the optimizations of the parameters of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955f9061",
   "metadata": {},
   "source": [
    "## A mathematical view on autograd\n",
    "\n",
    "Let's take a high-level mathematical helicopter view of our machine learning model and discuss it here as a *function*, with inputs and outputs.\n",
    "\n",
    "The inputs are defined by an *i*-dimensional vector $\\vec x$, with elements $x_{i}$. You can see them as the input features. We can then express the model, *M*, as a vector-valued function of the input and it will in a general case result in a vector output: *$\\vec y$ = $\\vec M$($\\vec x$)*.\n",
    "\n",
    "Since we'll be using autograd in the context of training, our output of interest will be the model's loss. \n",
    "\n",
    "The *loss function* *L($\\vec y$) = L($\\vec M$($\\vec x$))* is typically a **single-valued** scalar function of the model's output. This function expresses how far off our model's prediction was from a particular input's *true* output.  \n",
    "\n",
    "The goal of *training* a model is thus to  minimize the loss by adjusting the parameters of our model. For a perfect model this will mean that we want to tweak the parameters of our model in such a way, that our loss function will result in a 0 outcome. With a zero loss we have a perfect model that gives our *true* outputs. \n",
    "\n",
    "In reality our data, both input and output, will have some noise and we will not always get *true* outputs. Also our model is always an approximation of reality but not reality itself and rounding errors in our calculations will have a further impact, resulting in a model that does not give *true* outputs.\n",
    "\n",
    "To guide the tuning of the parameters to minimize the loss, we need a mathematical tool to guide us in the good direction, meaning down the slope. This mathematical tool to *minimize* the loss is the first derivative which we'll need to equal to 0: $\\frac{\\partial L}{\\partial x} = 0$. With the first derivative having a value of 0, we'll know that our model's parameters have been changed in such a way that we have reached a minimum. Depending on the loss-function, this minimum could then be global or a local minimum (if e.g. the loss-function is convex and is differentiable everywhere, then we know that when the gradient of the loss-function=0 then we have reached the global minimum).\n",
    "\n",
    "Mind that the loss is a function of the model's output and is not *directly* derived from the input. Off course the output is an *indirect* function of the input, through the model that is being applied to the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72919d4",
   "metadata": {},
   "source": [
    "So: $\\frac{\\partial L}{\\partial x}$ = $\\frac{\\partial {L({\\vec y})}}{\\partial x}$. By the chain rule of differential calculus, we have $\\frac{\\partial {L({\\vec y})}}{\\partial x}$ = $\\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial x}$ = $\\frac{\\partial L}{\\partial y}\\frac{\\partial M(x)}{\\partial x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7360061f",
   "metadata": {},
   "source": [
    "In this calculation it is the part of $\\frac{\\partial M(x)}{\\partial x}$ where things get complex. In a neural network we have a linear combination of inputs and weights on top of which we apply non-linear activation functions. In more complex Neural Networks, we can also apply other transformations like convolutions, max-poolings, ...  \n",
    "\n",
    "The partial derivatives of the model's outputs with respect to its inputs, $\\frac{\\partial M(x)}{\\partial x}$, can again be expanded using the chain-rule on the components of the network. This will result in needing to calculate all the local partial derivatives over every multiplied learning weight, every activation function, and every other mathematical transformation in the model. \n",
    "\n",
    "The final result will be a sum of the products of the local gradient of *every possible path* through the computational graph that ends with the variable whose gradient we are trying to measure.\n",
    "\n",
    "It are off-course the gradients of the parameters/weights of our model that we are interested in as these gradients will allows us to modify the parameters of the model, as they tell us *what direction to change each weight* so that the loss function gets closer to zero.\n",
    "\n",
    "For a neural net, the number of layers and the number of weights is growing into the 100's of millions and models get 'deeper' and deeper and so complexer and complexer. There will also be a need to calculate local derivatives for each parameter, where each parameter corresponds to a separate path through the model's computational graph and so the number of derivatives to calculate will go up exponentially with the depth of a neural network. \n",
    "\n",
    "It is here that autograd will help out in tracking the history of every computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462280d8",
   "metadata": {},
   "source": [
    "## An autograd example\n",
    "\n",
    "Let's start with a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c95e0395",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:34:55.782427Z",
     "start_time": "2022-12-06T09:34:55.775427Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1924cde",
   "metadata": {},
   "source": [
    "## **requires_grad** argument\n",
    "\n",
    "This argument will tell pytorch that it will need to calculate the gradients for this tensor\n",
    "later in your optimization steps.\n",
    "\n",
    "i.e. this is a variable in your model that you want to optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5ec2a5",
   "metadata": {},
   "source": [
    "The autograd package provides automatic differentiation for all operations on Tensors. This is done by adding `requires_grad = True` to all derivative operations on the tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7472e5e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:34:55.845426Z",
     "start_time": "2022-12-06T09:34:55.832434Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.linspace(0., 6., steps=10, requires_grad=True)\n",
    "y = x + 2\n",
    "\n",
    "\n",
    "x = torch.randn(4, requires_grad=True)\n",
    "\n",
    "y = x+2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ca5db4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T11:04:40.207587Z",
     "start_time": "2022-11-11T11:04:40.199573Z"
    }
   },
   "source": [
    "As **y** was created as a result of an operation (here an addition), it will get a `grad_fn` attribute, indicating what operation was performed on the input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8a832d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:34:55.877436Z",
     "start_time": "2022-12-06T09:34:55.861429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([-0.7020,  0.7491,  0.3764, -0.3933], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('x:',x) # created by the user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2858f80c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T11:30:38.474998Z",
     "start_time": "2022-11-11T11:30:38.461999Z"
    }
   },
   "source": [
    "Look at the last attribute of the **w** pytorch-tensor. The `requires_grad` attribute indicates that the variable **x** is a **leaf** node of our calculation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9118e957",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:34:55.908431Z",
     "start_time": "2022-12-06T09:34:55.892432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25961b96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T11:04:40.207587Z",
     "start_time": "2022-11-11T11:04:40.199573Z"
    }
   },
   "source": [
    "So `grad_fn` gives us a hint that when we execute the backpropagation step and compute gradients, what needs to be done.\n",
    "\n",
    "For `x.grad_fn` it returned None, indicating no action is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5efc0ed8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:34:55.940427Z",
     "start_time": "2022-12-06T09:34:55.925427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: tensor([1.2980, 2.7491, 2.3764, 1.6067], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('y:',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6ffb17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T11:30:38.474998Z",
     "start_time": "2022-11-11T11:30:38.461999Z"
    }
   },
   "source": [
    "Look again at the last attribute of the **y** pytorch-tensor. This `grad_fn` attribute indicates that this is a **non-leaf** node of our calculation graph and it indicates which operation needs to be performed when we have to backtrack when performing the backward step.\n",
    "\n",
    "The type of operation is stored in this `grad_fn` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb28eb47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:34:55.972429Z",
     "start_time": "2022-12-06T09:34:55.956430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x0000014CD084EEE0>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8edea19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T11:04:40.207587Z",
     "start_time": "2022-11-11T11:04:40.199573Z"
    }
   },
   "source": [
    "For `y.grad_fn` it returned that autograd will need to compute the derivative of $add(x)$ in the backward step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783aa0dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T10:57:30.042689Z",
     "start_time": "2022-11-11T10:57:30.042689Z"
    }
   },
   "source": [
    "Let's do some more operations on y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d012f73e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:34:56.020434Z",
     "start_time": "2022-12-06T09:34:56.005430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z: tensor([ 5.0546, 22.6726, 16.9414,  7.7444], grad_fn=<MulBackward0>)\n",
      "<MulBackward0 object at 0x0000014CD084ECD0>\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "print('z:',z) # the result of a multiplication of y with itself (squaring) and multiply by 3 \n",
    "print(z.grad_fn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09fedd25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:34:56.208432Z",
     "start_time": "2022-12-06T09:34:56.023425Z"
    }
   },
   "outputs": [],
   "source": [
    "# plt.plot(x.detach(), z.detach())    \n",
    "#       # we'll explain detach below, but here you must know that it allows to make a numpy copy of the pytorch tensor, \n",
    "#       # without affecting the gradient's calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e139e2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:34:56.224425Z",
     "start_time": "2022-12-06T09:34:56.210422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor(52.4129, grad_fn=<SumBackward0>)\n",
      "<SumBackward0 object at 0x0000014CD084ED90>\n"
     ]
    }
   ],
   "source": [
    "t = z.sum() # here t is a TENSOR of DIMENSION 0\n",
    "print('t:',t)\n",
    "print(t.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94355f9",
   "metadata": {},
   "source": [
    "So far no gradients have been calculated, but the model knows how it should calculate all these gradients based on the graf_fn attributes of each variable in each of the layers.\n",
    "\n",
    "When we finish our model computation - this is also called the forward pass - we can call `.backward()` and have all the gradients computed automatically.\n",
    "\n",
    "Let's start off easy and when you call `.backward()` on a tensor with no arguments, it expects the calling tensor to contain only a **single** element, as is the case when computing a loss function where we want to optimize 1 value (i.e. the loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d494e6f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T11:18:57.356694Z",
     "start_time": "2022-11-11T11:18:57.344692Z"
    }
   },
   "source": [
    "So let's take the next step and ask the system to calculate all the gradients, by means of `.backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f844fa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:34:56.240433Z",
     "start_time": "2022-12-06T09:34:56.227425Z"
    }
   },
   "outputs": [],
   "source": [
    "t.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec5c95f",
   "metadata": {},
   "source": [
    "The gradient for this tensor will be accumulated into the .grad attribute of the *original* tensor (so of x). It is the partial derivate of the output function w.r.t. the tensor x. Here x is seen as a leaf tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa50e8b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:34:56.256414Z",
     "start_time": "2022-12-06T09:34:56.242424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad:\n",
      " tensor([ 7.7882, 16.4946, 14.2582,  9.6402])\n"
     ]
    }
   ],
   "source": [
    "print('x.grad:\\n',x.grad) # dt/dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8d7f2e",
   "metadata": {},
   "source": [
    "For all other intermediate steps in our calculations - on non-leaf tensor - the .grad will not be populated and so will contain the value None (+ you'll get a warning if you do ask). If you do want the gradient to be calculated then you need to set the non-leaf tensor to have `.retain_grad()`. This is also indicated when you try to access the gradient of intermediate variables. Just have a look at the warning you get when uncommenting the below commented lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdef05bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:34:56.272415Z",
     "start_time": "2022-12-06T09:34:56.258420Z"
    }
   },
   "outputs": [],
   "source": [
    "# print('y.grad:\\n',y.grad) # dt/dy\n",
    "# print('z.grad:\\n',z.grad) # dt/dz\n",
    "# print('t.grad:\\n',t.grad) # dt/dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee4a0bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T11:47:36.040670Z",
     "start_time": "2022-11-11T11:47:36.024500Z"
    }
   },
   "source": [
    "There is more functionality included on the `grad_fn` function.\n",
    "\n",
    "Each `grad_fn` stored with our tensors allows you to walk the computation all the way back to its inputs with its `next_functions property`. \n",
    "\n",
    "We can see below that drilling down on this property on t shows us the gradient functions for all the prior tensors. Note that `x.grad_fn` is reported as None, indicating that this was an input to the function with no history of its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cc96267",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:34:56.288424Z",
     "start_time": "2022-12-06T09:34:56.275421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " t:\n",
      "<SumBackward0 object at 0x0000014CD08671F0>\n",
      "((<MulBackward0 object at 0x0000014CD0867370>, 0),)\n",
      "((<MulBackward0 object at 0x0000014CD08671F0>, 0), (None, 0))\n",
      "((<AddBackward0 object at 0x0000014CD0867280>, 0), (<AddBackward0 object at 0x0000014CD0867280>, 0))\n",
      "((<AccumulateGrad object at 0x0000014CD08671F0>, 0), (None, 0))\n",
      "\n",
      " z:\n",
      "<MulBackward0 object at 0x0000014CD08671F0>\n",
      "\n",
      " y:\n",
      "<AddBackward0 object at 0x0000014CD08671F0>\n",
      "\n",
      " x:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(' t:')\n",
    "print(t.grad_fn)\n",
    "print(t.grad_fn.next_functions)\n",
    "print(t.grad_fn.next_functions[0][0].next_functions)\n",
    "print(t.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print(t.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print('\\n z:')\n",
    "print(z.grad_fn)\n",
    "print('\\n y:')\n",
    "print(y.grad_fn)\n",
    "print('\\n x:')\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c19e99d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-11T11:25:38.479886Z",
     "start_time": "2022-11-11T11:25:38.466891Z"
    }
   },
   "source": [
    "### Exercise: Create a function that calculates the $exp(2*x^2*sin(x))$ over the range [-2, 2], with steps=25\n",
    "\n",
    "Use intermediate variables for each mathematical operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d40a8d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:34:56.304420Z",
     "start_time": "2022-12-06T09:34:56.291426Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38d4d054",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:34:56.320414Z",
     "start_time": "2022-12-06T09:34:56.306422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.0000, -1.8333, -1.6667, -1.5000, -1.3333, -1.1667, -1.0000, -0.8333,\n",
      "        -0.6667, -0.5000, -0.3333, -0.1667,  0.0000,  0.1667,  0.3333,  0.5000,\n",
      "         0.6667,  0.8333,  1.0000,  1.1667,  1.3333,  1.5000,  1.6667,  1.8333,\n",
      "         2.0000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b9ce9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T08:34:09.935365Z",
     "start_time": "2022-12-06T08:34:09.921369Z"
    }
   },
   "source": [
    "### Exercise: Call the appropriate function on the variable to calculate the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "585e4cdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:34:56.336415Z",
     "start_time": "2022-12-06T09:34:56.323423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad:\n",
      " tensor([2.7343e-03, 8.0899e-03, 2.4208e-02, 7.0817e-02, 1.9001e-01, 4.3878e-01,\n",
      "        8.2628e-01, 1.2166e+00, 1.3549e+00, 1.0997e+00, 6.0093e-01, 1.6387e-01,\n",
      "        0.0000e+00, 1.6691e-01, 6.9499e-01, 1.7762e+00, 4.0675e+00, 9.5081e+00,\n",
      "        2.3928e+01, 6.5504e+01, 1.9073e+02, 5.6104e+02, 1.5392e+03, 3.5214e+03,\n",
      "        5.6924e+03])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6159b7df",
   "metadata": {},
   "source": [
    "________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11528be0",
   "metadata": {},
   "source": [
    "## Autograd: code and function\n",
    "\n",
    "`requires_grad=True`    ->\tTensor will be tracked for gradient computation\n",
    "\n",
    "\n",
    "`.grad` ->\tStores the computed gradient\n",
    "\n",
    "\n",
    "`.backward()`   ->\tTriggers automatic differentiation\n",
    "\n",
    "\n",
    "`torch.no_grad()`   ->\tTemporarily disables autograd (useful for inference)\n",
    "\n",
    "\n",
    "`.detach()` ->\tCreates a tensor that shares data but doesnâ€™t track gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b0c36d",
   "metadata": {},
   "source": [
    "_______________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e9e390",
   "metadata": {},
   "source": [
    "## Turning Autograd On and Off\n",
    "\n",
    "There are situations where you will need fine-grained control over whether autograd is enabled. There are multiple ways to do this, depending on the situation.\n",
    "\n",
    "There are 3 ways to do so:\n",
    "\n",
    "    - Using requires_grad=True\n",
    "    - Using with torch.no_grad()\n",
    "    - Using .detach()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a1becb",
   "metadata": {},
   "source": [
    "### Using `requires_grad=True`\n",
    "The simplest is to change the `requires_grad` flag on a tensor directly.\n",
    "\n",
    "Let's try this on an input tensor `a` and see what happens to 2 other tensors `b1` and  `b2`, where one has the gradients and the other not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48c415ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:34:56.352413Z",
     "start_time": "2022-12-06T09:34:56.339421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], requires_grad=True)\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]], grad_fn=<MulBackward0>)\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 3, requires_grad=True)\n",
    "print(a)\n",
    "\n",
    "b1 = 2 * a\n",
    "print(b1)\n",
    "\n",
    "a.requires_grad = False\n",
    "b2 = 2 * a\n",
    "print(b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90e4246",
   "metadata": {},
   "source": [
    "In the cell above, we see that `b1` has a `grad_fn` (i.e., a traced computation history), which is what we expect, since it was derived from a tensor, `a`, that had autograd turned on. When we turn off autograd explicitly with `a.requires_grad = False`, computation history is no longer tracked, as we see when we compute `b2`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42099ab",
   "metadata": {},
   "source": [
    "### Using `torch.no_grad()`\n",
    "If you only need autograd turned off **temporarily**, a better way is to use the `torch.no_grad()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8456a04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:34:56.367413Z",
     "start_time": "2022-12-06T09:34:56.356421Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1:\n",
      " tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]], grad_fn=<AddBackward0>)\n",
      "c2:\n",
      " tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]])\n",
      "c3:\n",
      " tensor([[6., 6., 6.],\n",
      "        [6., 6., 6.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 3, requires_grad=True) * 2\n",
    "b = torch.ones(2, 3, requires_grad=True) * 3\n",
    "\n",
    "c1 = a + b\n",
    "print('c1:\\n', c1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    c2 = a + b\n",
    "    print('c2:\\n', c2)\n",
    "\n",
    "c3 = a * b\n",
    "print('c3:\\n', c3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7e9215",
   "metadata": {},
   "source": [
    "`torch.no_grad()` can also be used as a function or method decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01561ba1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:34:56.399416Z",
     "start_time": "2022-12-06T09:34:56.384417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1:\n",
      " tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]], grad_fn=<AddBackward0>)\n",
      "c2:\n",
      " tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "def add_tensors1(x, y):\n",
    "    return x + y\n",
    "\n",
    "@torch.no_grad()\n",
    "def add_tensors2(x, y):\n",
    "    return x + y\n",
    "\n",
    "\n",
    "a = torch.ones(2, 3, requires_grad=True) * 2\n",
    "b = torch.ones(2, 3, requires_grad=True) * 3\n",
    "\n",
    "c1 = add_tensors1(a, b)\n",
    "print('c1:\\n', c1)\n",
    "\n",
    "c2 = add_tensors2(a, b)\n",
    "print('c2:\\n', c2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d780db",
   "metadata": {},
   "source": [
    "There's a corresponding context manager, `torch.enable_grad()`, for turning autograd on when it isn't already. It may also be used as a decorator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a4ee28",
   "metadata": {},
   "source": [
    "### Using `.detach()`\n",
    "Finally, you may have a tensor that requires gradient tracking, but you want a copy that does not. For this we have the `Tensor` object's `detach()` method - it creates a copy of the tensor that is *detached* from the computation history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6bd5f7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:34:56.447412Z",
     "start_time": "2022-12-06T09:34:56.438423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0737, 0.8558, 0.6912, 0.8008, 0.4990], requires_grad=True)\n",
      "tensor([0.0737, 0.8558, 0.6912, 0.8008, 0.4990])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, requires_grad=True)\n",
    "y = x.detach()\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24586f7",
   "metadata": {},
   "source": [
    "We did this above when we wanted to graph some of our tensors. This is because `matplotlib` expects a NumPy array as input, and the implicit conversion from a PyTorch tensor to a NumPy array is not enabled for tensors with requires_grad=True. \n",
    "\n",
    "E.g. making a detached copy is necessary when you want to display your results with `matplotlib`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370e14c6",
   "metadata": {},
   "source": [
    "### Autograd and In-place Operations\n",
    "\n",
    "In every example in this notebook so far, we've used variables to capture the intermediate values of a computation. Autograd needs these intermediate values to perform gradient computations. \n",
    "\n",
    "**For this reason, you must be careful about using in-place operations when using autograd.**\n",
    "\n",
    "Doing so can destroy information you need to compute derivatives in the `backward()` call. PyTorch will even stop you if you attempt an in-place operation on leaf variable that requires autograd, as shown below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bd288b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T08:34:10.918467Z",
     "start_time": "2022-12-06T08:34:10.001373Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note: The following lines of code - when uncommented - throws a runtime error. This is expected.\n",
    "# a = torch.linspace(0., 2. * np.math.pi, steps=25, requires_grad=True)\n",
    "# torch.sin_(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf1bd81",
   "metadata": {},
   "source": [
    "## Autograd Profiler (Nice to know)\n",
    "\n",
    "Autograd tracks every step of your computation in detail. Such a computation history, combined with timing information, would make a handy profiler - and autograd has that feature baked in. Here's a quick example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4346201",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:35:12.488280Z",
     "start_time": "2022-12-06T09:35:12.354282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "    aten::mul        53.32%       4.000ms        53.32%       4.000ms       4.000us      11.039ms        50.72%      11.039ms      11.039us          1000  \n",
      "    aten::div        46.68%       3.502ms        46.68%       3.502ms       3.502us      10.726ms        49.28%      10.726ms      10.726us          1000  \n",
      "-------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 7.502ms\n",
      "Self CUDA time total: 21.765ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "run_on_gpu = False\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    run_on_gpu = True\n",
    "    \n",
    "x = torch.randn(2, 3, requires_grad=True)\n",
    "y = torch.rand(2, 3, requires_grad=True)\n",
    "z = torch.ones(2, 3, requires_grad=True)\n",
    "\n",
    "with torch.autograd.profiler.profile(use_cuda=run_on_gpu) as prf:\n",
    "    for _ in range(1000):\n",
    "        z = (z / x) * y\n",
    "        \n",
    "print(prf.key_averages().table(sort_by='self_cpu_time_total'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dcd42e",
   "metadata": {},
   "source": [
    "The profiler can also label individual sub-blocks of code, break out the data by input tensor shape, and export data as a Chrome tracing tools file. For full details of the API, see the [documentation](https://pytorch.org/docs/stable/autograd.html#profiler)\n",
    "\n",
    "## Advanced Topic: More Autograd Detail and the High-Level API (Nice to know)\n",
    "\n",
    "If you have a function with an n-dimensional input and m-dimensional output, $\\vec{y}=f(\\vec{x})$, the complete gradient is a matrix of the derivative of every output with respect to every input, called the *Jacobian:*\n",
    "\n",
    "\\begin{align}J=\\left(\\begin{array}{ccc}\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "   \\vdots & \\ddots & \\vdots\\\\\n",
    "   \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "   \\end{array}\\right)\\end{align}\n",
    "\n",
    "If you have a second function, $l=g\\left(\\vec{y}\\right)$ that takes m-dimensional input (that is, the same dimensionality as the output above), and returns a scalar output, you can express its gradients with respect to $\\vec{y}$ as a column vector, $v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}$ - which is really just a one-column Jacobian.\n",
    "\n",
    "More concretely, imagine the first function as your PyTorch model (with potentially many inputs and many outputs) and the second function as a loss function (with the model's output as input, and the loss value as the scalar output).\n",
    "\n",
    "If we multiply the first function's Jacobian by the gradient of the second function, and apply the chain rule, we get:\n",
    "\n",
    "$J^{T}\\cdot v=\\left(\\begin{array}{ccc}\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
    "   \\vdots & \\ddots & \\vdots\\\\\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "   \\end{array}\\right)\\left(\\begin{array}{c}\n",
    "   \\frac{\\partial l}{\\partial y_{1}}\\\\\n",
    "   \\vdots\\\\\n",
    "   \\frac{\\partial l}{\\partial y_{m}}\n",
    "   \\end{array}\\right)=\\left(\\begin{array}{c}\n",
    "   \\frac{\\partial l}{\\partial x_{1}}\\\\\n",
    "   \\vdots\\\\\n",
    "   \\frac{\\partial l}{\\partial x_{n}}\n",
    "   \\end{array}\\right)$\n",
    "\n",
    "The resulting column vector is the *gradient of the second function with respect to the inputs of the first* - or in the case of our model and loss function, the gradient of the loss with respect to the model inputs.\n",
    "\n",
    "**`torch.autograd` is an engine for computing these products.** This is how we accumulate the gradients over the learning weights during the backward pass.\n",
    "\n",
    "For this reason, the `backward()` call can *also* take an optional vector input. This vector represents a set of gradients over the tensor, which are multiplied by the Jacobian of the autograd-traced tensor that precedes it. Let's try a specific example with a small vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24481c6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:35:14.197019Z",
     "start_time": "2022-12-06T09:35:14.176015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1189.2156,   989.5829,   100.2688], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000: # calculate the L2 norm of the data (torch.sqrt(torch.sum(torch.pow(y, 2))) == y.data.norm())\n",
    "    y = y * 2\n",
    "\n",
    "print(y)\n",
    "# y.backward() # Uncommented, this would result in an error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0c868d",
   "metadata": {},
   "source": [
    "If we tried to call `y.backward()` now, - by uncommenting the line above - we'd get a runtime error and a message that gradients can only be *implicitly* computed for scalar outputs. For a multi-dimensional output, autograd expects us to provide gradients for those - in this case - three outputs that it can multiply into the Jacobian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33d8f406",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:35:15.366707Z",
     "start_time": "2022-12-06T09:35:15.346686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.1200e+02, 5.1200e+02, 5.1200e-02])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([1.0, 1.0, 0.0001], dtype=torch.float) # stand-in for gradients\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807523dd",
   "metadata": {},
   "source": [
    "(Note that the output gradients are all related to powers of two - which we'd expect from a repeated doubling operation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e0f50cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:35:16.869104Z",
     "start_time": "2022-12-06T09:35:16.859257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.5000, 2.6767, 2.8564, 3.0385, 3.2226, 3.4082, 3.5953, 3.7835])\n"
     ]
    }
   ],
   "source": [
    "# Another example\n",
    "\n",
    "dx = 0.1\n",
    "x = torch.arange(1, 1.8, dx, requires_grad=True)\n",
    "f = x**2 + torch.sqrt(x) # the gradient of this is 2*x + 0.5/sqrt(x)\n",
    "\n",
    "f.backward(torch.ones(f.shape))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16e3269",
   "metadata": {},
   "source": [
    "Also have a look at https://stackoverflow.com/questions/57248777/backward-function-in-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915ab20e",
   "metadata": {},
   "source": [
    "### The High-Level API (nice to know)\n",
    "\n",
    "There is an API on autograd that gives you direct access to important differential matrix and vector operations. In particular, it allows you to calculate the Jacobian and the *Hessian* matrices of a particular function for particular inputs. (The Hessian is like the Jacobian, but expresses all partial *second* derivatives.) It also provides methods for taking vector products with these matrices.\n",
    "\n",
    "Let's take the Jacobian of a simple function, evaluated for a 2 single-element inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0421e00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:35:21.565958Z",
     "start_time": "2022-12-06T09:35:21.550961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.4438]), tensor([0.8725]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[3.1172]]), tensor([[3.]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exp_adder(x, y):\n",
    "    return 2 * x.exp() + 3 * y\n",
    "\n",
    "inputs = (torch.rand(1), torch.rand(1)) # arguments for the function\n",
    "print(inputs)\n",
    "torch.autograd.functional.jacobian(exp_adder, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65090be",
   "metadata": {},
   "source": [
    "If you look closely, the first output should equal $2e^x$ (since the derivative of $e^x$ is $e^x$), and the second value should be 3.\n",
    "\n",
    "You can, of course, do this with higher-order tensors. E.g. for 2 1 dimensional vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "abcd9595",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T08:34:39.440093Z",
     "start_time": "2022-12-06T08:34:39.420090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.1464, 0.2125, 0.7914]), tensor([0.9378, 0.4660, 0.1729]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[2.3152, 0.0000, 0.0000],\n",
       "         [0.0000, 2.4735, 0.0000],\n",
       "         [0.0000, 0.0000, 4.4129]]),\n",
       " tensor([[3., 0., 0.],\n",
       "         [0., 3., 0.],\n",
       "         [0., 0., 3.]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = (torch.rand(3), torch.rand(3)) # arguments for the function\n",
    "print(inputs)\n",
    "torch.autograd.functional.jacobian(exp_adder, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f2cfee",
   "metadata": {},
   "source": [
    "The `torch.autograd.functional.hessian()` method works identically (assuming your function is twice differentiable), but returns a matrix of all second derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5d680b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T08:34:40.165011Z",
     "start_time": "2022-12-06T08:34:40.148996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.8534]), tensor([0.0284]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((tensor([[4.6952]]), tensor([[0.]])), (tensor([[0.]]), tensor([[0.]])))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = (torch.rand(1), torch.rand(1)) # arguments for the function\n",
    "print(inputs)\n",
    "torch.autograd.functional.hessian(exp_adder, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7e72aa",
   "metadata": {},
   "source": [
    "There is also a function to directly compute the vector-Jacobian product (vjb), if you provide the vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c4c844b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T08:34:40.892559Z",
     "start_time": "2022-12-06T08:34:40.870394Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1672.8020, -118.8002,  228.2352]),\n",
       " tensor([1.0240e+02, 1.0240e+03, 1.0240e-01]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def do_some_doubling(x):\n",
    "    y = x * 2\n",
    "    while y.data.norm() < 1000:\n",
    "        y = y * 2\n",
    "    return y\n",
    "\n",
    "inputs = torch.randn(3)\n",
    "my_gradients = torch.tensor([0.1, 1.0, 0.0001])\n",
    "torch.autograd.functional.vjp(do_some_doubling, inputs, v=my_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12423be3",
   "metadata": {},
   "source": [
    "The `torch.autograd.functional.jvp()` method performs the same matrix multiplication as `vjp()` with the operands reversed. The `vhp()` and `hvp()` methods do the same for a vector-Hessian product.\n",
    "\n",
    "For more information, including preformance notes on the [docs for the functional API](https://pytorch.org/docs/stable/autograd.html#functional-higher-level-api)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e035b8a9",
   "metadata": {},
   "source": [
    "## When exaclty you need and don't need Autograd?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143b1d9c",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85686ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8943e2e4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdf42c7b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c145640",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "interpreter": {
   "hash": "360887b9cab5489ef98dcd0c24153a5025959100434b1781469b024af9ed3ee4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
