{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Validation Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Overview: why PyTorch uses explicit loops\n",
    "\n",
    "TensorFlow's model.fit is convenient: a high-level API that hides training details. PyTorch gives you lower-level control by design: you explicitly write the training loop. This is powerful for debugging, custom losses, per-batch logic, dynamic graphs, complex regularization, and research experiments.\n",
    "\n",
    "Writing the loop yourself forces you to understand the lifecycle of data, model, gradient computation, and optimization. It looks more code initially, but it's explicit and flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Quick PyTorch \n",
    "\n",
    "Tensors: `torch.Tensor` is the central object. Move to GPU with `.to(device)` or `.cuda()`.\n",
    "\n",
    "Autograd: When you perform operations on tensors with `requires_grad=True`, PyTorch builds a dynamic computation graph. Calling `.backward()` computes gradients and stores them in .grad.\n",
    "\n",
    "Modules: `torch.nn.Module` is a base class for models. Use `nn.Sequential`, or define class `MyModel(nn.Module)` and implement `forward(self, x)`.\n",
    "\n",
    "Optimizers: From `torch.optim`. They update the model parameters which are returned by `model.parameters()`.\n",
    "\n",
    "DataLoader: Wraps a Dataset to provide minibatches, shuffling, and parallel data loading. Use `torch.utils.data.DataLoader`.\n",
    "\n",
    "Device management: device = `torch.device('cuda' if torch.cuda.is_available() else 'cpu')`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEPS NEEDED TO TRAIN A MODEL:\n",
    "\n",
    "1️⃣ Have our data, moving to GPU/CPU is optional\n",
    "\n",
    "`inputs, targets = inputs.to(device), targets.to(device)`\n",
    "\n",
    "2️⃣ Zero out old gradients\n",
    "\n",
    "`optimizer.zero_grad()`\n",
    "\n",
    "3️⃣ Forward pass (compute predictions)\n",
    "\n",
    "`outputs = model(inputs)`\n",
    "\n",
    "4️⃣ Compute the loss\n",
    "\n",
    "`loss = criterion(outputs, targets)`\n",
    "\n",
    "5️⃣ Backward pass (compute gradients)\n",
    "\n",
    "`loss.backward()`  \n",
    "\n",
    "6️⃣ Update weights\n",
    "\n",
    "`optimizer.step()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal but complete PyTorch training + validation example (CIFAR10)\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# ------------------- Helpers -------------------\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Device -------------------\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "\n",
    "# ------------------- Data -------------------\n",
    "\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "])\n",
    "\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "])\n",
    "\n",
    "\n",
    "train_ds = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "val_ds = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_val)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=256, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Model -------------------\n",
    "\n",
    "\n",
    "# Simple model — use torchvision.models for more serious work\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 8, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, num_classes),\n",
    ")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "model = SimpleCNN(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Loss, Optimizer, Scheduler -------------------\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, scaler=None, grad_clip=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, targets in tqdm(dataloader, desc='train', leave=False):\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        if scaler is not None:\n",
    "            # mixed-precision\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "            scaler.scale(loss).backward()\n",
    "            if grad_clip is not None:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            if grad_clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += float(loss.item()) * inputs.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        running_correct += (preds == targets).sum().item()\n",
    "        total += inputs.size(0)\n",
    "\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = running_correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    total = 0\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(dataloader, desc='val', leave=False):\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "\n",
    "            running_loss += float(loss.item()) * inputs.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            running_correct += (preds == targets).sum().item()\n",
    "            total += inputs.size(0)\n",
    "\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = running_correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Training loop -------------------\n",
    "\n",
    "\n",
    "num_epochs = 3\n",
    "scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "best_val_acc = 0.0\n",
    "history = defaultdict(list)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device, scaler=scaler, grad_clip=5.0)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "\n",
    "    print(f\" train_loss: {train_loss:.4f} train_acc: {train_acc:.4f}\")\n",
    "    print(f\" val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}\")\n",
    "\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'epoch': epoch}, 'best_checkpoint.pth')\n",
    "        print(' Saved best checkpoint')\n",
    "\n",
    "\n",
    "print('Training finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Detailed training loop explained\n",
    "\n",
    "Core steps per training batch:\n",
    "\n",
    "`model.train()` — enable training mode (affects dropout, batchnorm, etc.)\n",
    "\n",
    "Move inputs and targets to device: `inputs = inputs.to(device)`\n",
    "\n",
    "`optimizer.zero_grad()` — clear gradients from previous batch\n",
    "\n",
    "Forward: `outputs = model(inputs)`\n",
    "\n",
    "Compute loss: `loss = criterion(outputs, targets)`\n",
    "\n",
    "Backward: `loss.backward()` (or `scaler.scale(loss).backward()` for AMP)\n",
    "\n",
    "Optionally clip gradients\n",
    "\n",
    "`optimizer.step()` — update parameters\n",
    "\n",
    "Update any metrics & accumulate loss\n",
    "\n",
    "Why `zero_grad()`? PyTorch accumulates gradients by default — each backward() adds to .grad.\n",
    "\n",
    "Why `model.train()` vs `model.eval()`? Certain layers like dropout and batchnorm behave differently: `train()` enables training behavior; `eval()` freezes it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Validation loop, metrics, and torch.no_grad()\n",
    "\n",
    "Validation should not modify model parameters or track gradients. Use with `torch.no_grad()`: to disable gradient tracking (saves memory and compute). Also call `model.eval()` to set evaluation mode.\n",
    "\n",
    "Compute metrics (accuracy, precision/recall, F1) as required. For multi-class classification, argmax on logits is simple accuracy.\n",
    "\n",
    "Example metrics: running loss, accuracy, confusion matrix; for regression: MAE, MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn!\n",
    "\n",
    "\n",
    "\n",
    "Select a dataset from internet and code a Pytorch model on it, you can choose if it will be a regression or classification task!\n",
    "\n",
    "\n",
    "The goal is to get used to the Pytorch way of coding"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "460.8px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
